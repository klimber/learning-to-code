{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyaSGq65woLh",
        "colab_type": "text"
      },
      "source": [
        "![Codenation](https://forum.codenation.com.br/uploads/default/original/2X/2/2d2d2a9469f0171e7df2c4ee97f70c555e431e76.png)\n",
        "\n",
        "__Autor__: Kazuki Yokoyama (kazuki.yokoyama@ufrgs.br)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi4xZxcfBA2U",
        "colab_type": "text"
      },
      "source": [
        "# _Feature engineering_\n",
        "\n",
        "![cover](https://venturebeat.com/wp-content/uploads/2018/07/feature_engineering.jpg?resize=680%2C198&strip=all)\n",
        "\n",
        "Neste módulo, trabalharemos a engenharia de _features_, que consiste em preparar os nossos dados para alimentar os algoritmos de ML adequadamente. Ao contrário do mundo dos tutoriais, na vida real os dados dificilmente estarão prontos para serem consumidos. Grande parte do tempo de um projeto de ML é gasto com a engenharia de _features_, e quanto melhor a qualidade desta etapa, maiores são as chances de melhores resultados nas etapas seguintes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAxxSlo3QrZV",
        "colab_type": "text"
      },
      "source": [
        "## Importação das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMxYy1NkQwW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import functools\n",
        "from math import sqrt\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as sct\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_digits, fetch_20newsgroups\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import (\n",
        "    CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        ")\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import (\n",
        "    OneHotEncoder, Binarizer, KBinsDiscretizer,\n",
        "    MinMaxScaler, StandardScaler, PolynomialFeatures\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNbPRHkKQyv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Algumas configurações para o matplotlib.\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.core.pylabtools import figsize\n",
        "\n",
        "\n",
        "figsize(12, 12)\n",
        "\n",
        "sns.set()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8onCO86Q2Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIEVdatWDh3Z",
        "colab_type": "text"
      },
      "source": [
        "## _One-hot encoding_\n",
        "\n",
        "Até aqui, nós praticamente ignoramos a existência de variáveis categóricas. Focamos nas variáveis numéricas porque elas são simples de lidar e bastante comuns. Ainda assim, variáveis categóricas são encontradas facilmente e precisamos de uma forma de trabalhar com elas.\n",
        "\n",
        "Uma das formas mais simples de representação de variáveis categóricas é através do método chamado _one-hot enconding_. Com ele, uma variável categórica com $h$ categorias é transformada em $h$ novas variáveis binárias (0 ou 1), onde a presença do 1 (_hot_) significa que aquela observação pertence àquela categoria, e 0 (_cold_) que não pertence. Veja um exemplo abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1zv6xPDk4ym",
        "colab_type": "code",
        "outputId": "b9b41a48-556d-44e1-f142-708bae7a2d02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "rows = 100\n",
        "\n",
        "height = np.random.normal(loc=1.70, scale=0.2, size=rows).round(3)\n",
        "score = np.random.normal(loc=7, scale=1, size=rows).round(2)\n",
        "courses = [\"Math\", \"Physics\", \"Biology\"]\n",
        "course = np.random.choice(courses, size=rows)\n",
        "\n",
        "data = pd.DataFrame({\"Height\": height, \"Score\": score, \"Course\": course})\n",
        "\n",
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK_6LysZP6Lw",
        "colab_type": "text"
      },
      "source": [
        "Criamos um _data set_ que contém duas variáveis numéricas (`Height` e `Score`) e uma variável categórica (`Course`). Nosso objetivo com o _one-hot encoding_ é transformar a variável `Course` em uma sequência de variáveis numéricas binárias, cada uma descrevendo uma classe da variável. Neste caso, como temos três categorias para `Course` (Biology, Physics e Math), teremos três novas variáveis binárias.\n",
        "\n",
        "Vamos treinar esse _encoder_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDpY6XcNmYlw",
        "colab_type": "code",
        "outputId": "5fda81c9-000d-4557-cb3f-22d012b3e548",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "one_hot_encoder = OneHotEncoder(sparse=False, dtype=np.int)\n",
        "\n",
        "#one_hot_encoder.fit(data[[\"Course\"]])\n",
        "\n",
        "#course_encoded = one_hot_encoder.transform(...)\n",
        "\n",
        "course_encoded = one_hot_encoder.fit_transform(data[[\"Course\"]])\n",
        "\n",
        "course_encoded[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-O0cMCyQqk4",
        "colab_type": "text"
      },
      "source": [
        "A saída é um `np.ndarray` com formato `(n, h)`, onde `n` é o número de observações no _data set_ e `h` é o número de categorias da variável codificada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP_QsDI6REl_",
        "colab_type": "code",
        "outputId": "10a0faf0-b05f-4ad8-f79d-7642d15862a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "course_encoded.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoRT2AR8RHNl",
        "colab_type": "text"
      },
      "source": [
        "No atributo `categories_` do _encoder_, temos as categorias da variável:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziGE3VCinqM7",
        "colab_type": "code",
        "outputId": "2c77ac8b-ba1b-4479-97aa-b59cff8b78bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "one_hot_encoder.categories_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8V2WMjmRUkw",
        "colab_type": "text"
      },
      "source": [
        "Podemos criar as novas colunas que descrevem cada categoria. Repare que, para qualquer linha, apenas uma das colunas contém um 1, indicando a qual categoria aquela observação pertence. Isso acontece, obviamente, se as categorias forem mutuamente exclusivas (uma observação não pode pertencer a mais de uma categoria simultaneamente)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGepWPRFoqc0",
        "colab_type": "code",
        "outputId": "dc6a6dff-007d-4f66-cbfb-2aad4c8a7448",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "columns_encoded = one_hot_encoder.categories_[0]\n",
        "\n",
        "data_encoded = pd.concat([data, pd.DataFrame(course_encoded, columns=columns_encoded)], axis=1)\n",
        "\n",
        "data_encoded.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIiVR7P4SHXz",
        "colab_type": "text"
      },
      "source": [
        "Como você deve imaginar, a maior parte da matriz retornada é composta por zeros, sendo apenas alguns elementos compostos de um. Dizemos que essa matriz é __esparsa__. É um grande desperdício de memória trabalhar diretamente como uma matriz esparsa assim. Por isso, o _default_ do `OneHotEncoder` é retornar uma `sparse matrix` do NumPy, economizando espaço em memória:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muGSmJckraf3",
        "colab_type": "code",
        "outputId": "c8957d2b-68c4-4722-80ea-5e241c479a88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "one_hot_encoder_sparse = OneHotEncoder(sparse=True) # sparse=True é o default.\n",
        "\n",
        "course_encoded_sparse = one_hot_encoder_sparse.fit_transform(data[[\"Course\"]])\n",
        "\n",
        "course_encoded_sparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOYl0Lx8TPJm",
        "colab_type": "text"
      },
      "source": [
        "Para acessar os dados dessa matriz, podemos convertê-la para um _array_ não esparso:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtUziaQmrqTN",
        "colab_type": "code",
        "outputId": "bb7920ae-69a0-4543-97da-b1fc2746ddd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "course_encoded_sparse.toarray()[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHGmVXu1uEvM",
        "colab_type": "text"
      },
      "source": [
        "## Binarização (_Binarization_)\n",
        "\n",
        "Binarização é o processo de discretizar uma variável numérica em dois níveis com base em um _threshold_. Isso pode ser útil, por exemplo, para tornar uma variável numérica contínua em uma variável binária alvo de duas classes (positiva ou negativa).\n",
        "\n",
        "No exemplo abaixo, vamos separar a variável `Height` em dois grupos, utilizando 1.80 m como _threshold_ de separação. Observações que possuam menos de 1.80 m terão valor 0, enquanto aquelas com mais de 1.80 m terão valor 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeGrPpyWPcOw",
        "colab_type": "code",
        "outputId": "edb6b4c4-97e9-4914-f952-aa60c6dbbbc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "tall = (data_encoded.Height > 1.80)\n",
        "\n",
        "tall[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94vcsMVguGvG",
        "colab_type": "code",
        "outputId": "b2b15447-7399-4309-b18a-3de5a183a41e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "binarizer = Binarizer(threshold=1.80).fit(data_encoded[[\"Height\"]])\n",
        "\n",
        "height_binary = binarizer.transform(data_encoded[[\"Height\"]])\n",
        "\n",
        "height_binary[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oND_xnxRV8wZ",
        "colab_type": "text"
      },
      "source": [
        "O `Binarizer` tem como saída uma matriz binária numérica. Podemos transformá-la em um vetor de _bool_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXbf50-4vdDR",
        "colab_type": "code",
        "outputId": "2f7dba40-f513-491a-e072-743ac0a8c88f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "height_bool = pd.DataFrame(height_binary.flatten().astype(bool), columns=[\"Tall\"])\n",
        "\n",
        "height_bool.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn9Gs9DhWNvi",
        "colab_type": "text"
      },
      "source": [
        "Vamos adicionar a nova variável `Tall`, que indica se a pessoa é alta (> 1.80 m), ao nosso _data set_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjOV0WlJy7DY",
        "colab_type": "code",
        "outputId": "af316c4b-4931-44cb-a4af-4fa51b3c93fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "data_encoded = pd.concat([data_encoded, height_bool], axis=1)\n",
        "\n",
        "data_encoded.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tOdmnNi23p4",
        "colab_type": "text"
      },
      "source": [
        "## Discretização (_Binning_)\n",
        "\n",
        "Discretização, como o nome diz, é o processo de discretizar ou separar em intervalos contínuos uma variável numérica. Isso pode ser útil para converter uma variável numérica em categórica, quando o valor exato numérico não for tão importante quanto o intervalo onde ele se encontra.\n",
        "\n",
        "Podemos criar _bins_ (_buckets_ ou intervalos) que contenham aproximadamente a mesma quantidade de observações, utilizando a estratégia `quantile` ou que sejam igualmente espaçados com a estratégia `uniform`.\n",
        "\n",
        "No exemplo a seguir, criamos quatro intervalos da variável `Score` com a estratégia `quantile`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xir4K6i522ZQ",
        "colab_type": "code",
        "outputId": "e902850a-d3dc-4d97-a80f-ad3dad1bb1a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "discretizer = KBinsDiscretizer(n_bins=4, encode=\"ordinal\", strategy=\"quantile\")\n",
        "\n",
        "discretizer.fit(data_encoded[[\"Score\"]])\n",
        "\n",
        "score_bins = discretizer.transform(data_encoded[[\"Score\"]])\n",
        "\n",
        "score_bins[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hrP6E4xYXCs",
        "colab_type": "text"
      },
      "source": [
        "Os limites dos intervalos estão disponíveis no atributo `bin_edges_`. Isso pode ser útil para criarmos _labels_ para colunas do _data set_ por exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScCmeNtn3-fF",
        "colab_type": "code",
        "outputId": "be1003a5-2d28-42d6-e76d-bc349e957e95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "discretizer.bin_edges_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGl5ONq2Yk7r",
        "colab_type": "text"
      },
      "source": [
        "A função `get_interval()` abaixo facilita a criação de _labels_ indicativas dos intervalos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvB70_vd4fSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_interval(bin_idx, bin_edges):\n",
        "  return f\"{np.round(bin_edges[bin_idx], 2):.2f} ⊢ {np.round(bin_edges[bin_idx+1], 2):.2f}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn3eqHFbYtfm",
        "colab_type": "text"
      },
      "source": [
        "Cada um dos intervalos mostrados abaixo deve possuir aproximadamente a mesma quantidade de observações:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX59pepN5ZQQ",
        "colab_type": "code",
        "outputId": "d5b3d4dc-c969-44cb-fa34-e31fad2dd818",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "bin_edges_quantile = discretizer.bin_edges_[0]\n",
        "\n",
        "print(f\"Bins quantile\")\n",
        "print(f\"interval: #elements\\n\")\n",
        "for i in range(len(discretizer.bin_edges_[0])-1):\n",
        "    print(f\"{get_interval(i, bin_edges_quantile)}: {sum(score_bins[:, 0] == i)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ0fli3IY2G6",
        "colab_type": "text"
      },
      "source": [
        "A _Series_ abaixo mostra alguns dos intervalos para os quais as observações foram encaixadas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZMBYjqR5-H6",
        "colab_type": "code",
        "outputId": "cba541dc-9f9e-48d8-eb87-fa54440ca353",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "score_intervals = pd.Series(score_bins.flatten().astype(np.int)).apply(get_interval, args=(bin_edges_quantile,))\n",
        "\n",
        "score_intervals.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gWE7IU6Y_9q",
        "colab_type": "text"
      },
      "source": [
        "Também podemos criar uma nova variável, `Score_interval`, no nosso _data set_ com os intervalos (que agora são categorias):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fomFOQbVA8eS",
        "colab_type": "code",
        "outputId": "1f065c4f-6da4-43ad-ebb7-b58706595871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "data_encoded = pd.concat([data_encoded, pd.DataFrame(score_intervals, columns=[\"Score_interval\"])], axis=1)\n",
        "\n",
        "data_encoded.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LldlZ92lZN1k",
        "colab_type": "text"
      },
      "source": [
        "Como dito, podemos utilizar a estratégia `uniform` para criar _bins_ igualmente espaçados, independente do número de observações que cada um possui. Também podemos especificar o tipo de codificação utilizada. No caso a seguir, utilizamos `encode=onehot-dense` para informar que queremos que a saída seja codificada como o _one-hot encode_ visto anteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6L1qXuW-v-n",
        "colab_type": "code",
        "outputId": "956f9e9f-67ba-436f-f457-889ee2d1f3db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "discretizer_uniform = KBinsDiscretizer(n_bins=4, encode=\"onehot-dense\", strategy=\"uniform\")\n",
        "\n",
        "discretizer_uniform.fit(data_encoded[[\"Score\"]])\n",
        "\n",
        "score_bins_uniform = discretizer_uniform.transform(data_encoded[[\"Score\"]]).astype(np.int)\n",
        "\n",
        "score_bins_uniform[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YapI8RuMZZfM",
        "colab_type": "text"
      },
      "source": [
        "Note como agora os intervalos são ligeiramente diferentes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8gW9k-w-_CC",
        "colab_type": "code",
        "outputId": "731fca86-f052-4a93-e5bf-e13eec18ac8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bin_edges_uniform = discretizer_uniform.bin_edges_[0]\n",
        "\n",
        "bin_edges_uniform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieyy46EJAnb6",
        "colab_type": "code",
        "outputId": "99835fa9-8003-4060-afae-2c4de66685ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "score_intervals_columns = [get_interval(i, bin_edges_uniform) for i in range(4)]\n",
        "\n",
        "print(f\"Bins uniform\")\n",
        "print(f\"interval: #elements\\n\")\n",
        "for i in range(len(discretizer_uniform.bin_edges_[0])-1):\n",
        "    print(f\"{get_interval(i, bin_edges_uniform)}: {sum(score_bins_uniform[:, i])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuWi-1U4Zzf_",
        "colab_type": "text"
      },
      "source": [
        "Podemos adicionar as novas variáveis binárias no _data set_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-v3UgiQB87S",
        "colab_type": "code",
        "outputId": "ad22d68f-c0e8-4a91-8838-842e7e2f5041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "data_encoded = pd.concat([data_encoded, pd.DataFrame(score_bins_uniform, columns=score_intervals_columns)], axis=1)\n",
        "\n",
        "data_encoded.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD8WM_-yzqSc",
        "colab_type": "text"
      },
      "source": [
        "## Normalização (_Scaling_)\n",
        "\n",
        "Normalização é o processo de colocar uma variável numérica em uma escala pré-determinada, geralmente $[0, 1]$, mas também é comum ser $[-1, 1]$.\n",
        "\n",
        "Para colocar no intervalo $[0, 1]$, basta subtrair cada valor da valor mínimo e dividir pela diferença do valor máximo e mínimo:\n",
        "\n",
        "$$x_{\\text{scaled}} = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}$$\n",
        "\n",
        "Abaixo, escalamos a variável `Score` no intervalo $[0, 1]$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMM2mu-Qzwnv",
        "colab_type": "code",
        "outputId": "5c60c83b-13bf-431d-e77e-a2fb2e8af317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "minmax_scaler = MinMaxScaler(feature_range=(0, 1)) # Default feature_scale é (0, 1).\n",
        "\n",
        "minmax_scaler.fit(data_encoded[[\"Score\"]])\n",
        "\n",
        "score_normalized = minmax_scaler.transform(data_encoded[[\"Score\"]])\n",
        "\n",
        "score_normalized[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPr-37M2UBj4",
        "colab_type": "code",
        "outputId": "dc170301-56af-4cab-da7c-307c5cbb94a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "score_normalized.min(), score_normalized.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et6m_2Bbbq-n",
        "colab_type": "text"
      },
      "source": [
        "Adicionamos a variável `Score` normalizada ao nosso _data set_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaYvCQtK0fzi",
        "colab_type": "code",
        "outputId": "9f8ccb6c-d0b7-4445-96c9-490f284f2357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "data_encoded = pd.concat([data_encoded, pd.DataFrame(score_normalized.flatten(), columns=[\"Score_normalized\"])], axis=1)\n",
        "\n",
        "data_encoded.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7-msElsbveR",
        "colab_type": "text"
      },
      "source": [
        "Para avaliar se os valores encontrados conferem, podemos utilizar a função `normalize` abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAfUGaFc061d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(x, xmin, xmax):\n",
        "  return (x - xmin)/(xmax - xmin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXywxNX-b-0K",
        "colab_type": "text"
      },
      "source": [
        "A função `partial()` do módulo `functools` (_builtin_ do Python) permite \"congelar\" alguns parâmetros da função passaga como argumento, facilitando a invocação desta função quando tais parâmetros são constantes. No caso abaixo, \"congelamos\" os argumentos `xmin` e `xmax` da função `normalize()` com os valores mínimo e máximo da variável `Score`, respectivamente. Nas invocações subsequentes de `normalize` não precisaremos passar esses argumentos, somente o argumento \"não congelado\" `x`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAlpigp21OVx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalize_score = functools.partial(normalize,\n",
        "                                    xmin=data_encoded.Score.min(),\n",
        "                                    xmax=data_encoded.Score.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhR0rwUIctTa",
        "colab_type": "text"
      },
      "source": [
        "O valor abaixo realmente confere com aquele encontrado pelo `MinMaxScaler`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMfk3jrU1mQV",
        "colab_type": "code",
        "outputId": "f9851c0d-9446-4f10-874e-cdba22b43722",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "normalize_score(data_encoded.Score[0]).round(6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEcSQzWJ2Yum",
        "colab_type": "text"
      },
      "source": [
        "## Padronização (_Standardization_)\n",
        "\n",
        "Padronização é o processo de tornar a variável com média zero e variância um. Esse processo não deve ser confundido com a normalização descrita acima.\n",
        "\n",
        "O processo é simples, basta subtrair a média dos dados de cada observação e dividi-los pelo desvio-padrão:\n",
        "\n",
        "$$x_{\\text{standardized}} = \\frac{x - \\bar{x}}{s}$$\n",
        "\n",
        "onde $\\bar{x}$ indica a média amostral e $s$ o desvio-padrão amostral."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXYXezCNdYue",
        "colab_type": "text"
      },
      "source": [
        "No exemplo abaixo, padronizamos a variável `Score`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qfhs3Eaq2dGV",
        "colab_type": "code",
        "outputId": "572aae65-5460-44d1-8134-dbc26f82e2d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "standard_scaler = StandardScaler()\n",
        "\n",
        "standard_scaler.fit(data_encoded[[\"Score\"]])\n",
        "\n",
        "score_standardized = standard_scaler.transform(data_encoded[[\"Score\"]])\n",
        "\n",
        "score_standardized[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJJucIQddgME",
        "colab_type": "text"
      },
      "source": [
        "E adicionamos a variável padronizada ao nosso _data set_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAndWLe13RSr",
        "colab_type": "code",
        "outputId": "4a6231c1-f459-4307-ad14-24c4e46760cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "data_encoded = pd.concat([data_encoded, pd.DataFrame(score_standardized.flatten(), columns=[\"Score_standardized\"])], axis=1)\n",
        "\n",
        "data_encoded.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SgwGLgOdk5Q",
        "colab_type": "text"
      },
      "source": [
        "Note que, ao contrário da variável normalizada, é possível ter valores negativos e positivos, menores e maiores que um. Isso é bem óbvio, pois os dados agora têm média 0 e variância 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0E9fwo93h9w",
        "colab_type": "code",
        "outputId": "2d9d5cdf-181b-4ca1-bea7-b382bf738ebd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_encoded.Score_standardized.mean(), data_encoded.Score_standardized.var()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av0cwG_Qd3Ow",
        "colab_type": "text"
      },
      "source": [
        "Novamente, para avaliar os resultados obtidos, podemos escrever nossa própria função de padronização:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khwEkoks3-cS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def standardize(x, xmean, xstd):\n",
        "  return (x - xmean)/xstd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14w3018J4Gwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "standardize_score = functools.partial(standardize,\n",
        "                                      xmean=data_encoded.Score.mean(),\n",
        "                                      xstd=data_encoded.Score.std())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAGxoUK5d-22",
        "colab_type": "text"
      },
      "source": [
        "Como esperado, o valor confere com o encontrado:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpaNVzOy4aCL",
        "colab_type": "code",
        "outputId": "fa0f42f0-32a5-48f4-f8d7-724350cdca86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "standardize_score(data_encoded.Score[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tO4OOJK7NY1",
        "colab_type": "text"
      },
      "source": [
        "## Criando um _Pipeline_\n",
        "\n",
        "Todo esse processo de transformar os dados pode ser bastante trabalhoso e entendiante. Para facilitar as coisas, o sklearn dispõe de um mecanismo de _pipeline_ que funciona como ao esteira de uma linha de montagem. Cada etapa desse _pipeline_ é uma transformação nos dados, de forma que, ao final do _pipeline_, temos os dados totalmente transformados. A vantagem é que agora especificamos todas as etapas, ou transformações, de uma só vez, e podemos reaproveitar esse _pipeline_ no futuro."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1LyaI0-B2hV",
        "colab_type": "code",
        "outputId": "011176a0-ec92-4122-9fc4-3b3d0a3118c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86on9pLMeidf",
        "colab_type": "text"
      },
      "source": [
        "Para evitar bagunçar com nosso _data set_ original, criamos uma cópia (rasa) dele:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdA8euCcZeq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_missing = data.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snDUyWqEenh8",
        "colab_type": "text"
      },
      "source": [
        "E para tornar o exemplo mais interessante, adicionamos (ou removemos?) dados faltantes ao _data set_. Isso porque uma das transformações úteis que podemos aplicar no _pipeline_ é justamente a imputação de dados, ou seja, preencher dados faltantes.\n",
        "\n",
        "As variáveis numéricas faltantes são representadas por `np.nan`, enquanto a variável categórica é representada pela classe `Unknown`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkVnbFAKS_fF",
        "colab_type": "code",
        "outputId": "6ba74eb6-0d60-419a-c39a-dd165cd49b60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "unknown_height_idx = pd.Index(np.random.choice(data_missing.index, 10, replace=False))\n",
        "unknown_score_idx = pd.Index(np.random.choice(data_missing.index, 10, replace=False))\n",
        "unknown_course_idx = pd.Index(np.random.choice(data_missing.index, 10, replace=False))\n",
        "\n",
        "data_missing.loc[unknown_height_idx, \"Height\"] = np.nan\n",
        "data_missing.loc[unknown_score_idx, \"Score\"] = np.nan\n",
        "data_missing.loc[unknown_course_idx, \"Course\"] = \"Unknown\"\n",
        "\n",
        "data_missing_idx = unknown_height_idx | unknown_score_idx | unknown_course_idx\n",
        "\n",
        "data_missing.loc[data_missing_idx].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmUJS9SzfC9Y",
        "colab_type": "text"
      },
      "source": [
        "Criamos o _pipeline_ com as seguintes etapas:\n",
        "\n",
        "1. Faça imputação dos dados, preenchendo os dados faltantes com a mediana dos dados presentes.\n",
        "2. Faça a normalização dos dados no intervalo _default_ $[0, 1]$.\n",
        "3. Crie novas variáveis através da expansão polinomial da variável original."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ypslSlEhGBr",
        "colab_type": "text"
      },
      "source": [
        "O `Pipeline` recebe uma lista de transformações representadas por tuplas de dois elementos. Cada tupla contém:\n",
        "\n",
        "* O nome para a etapa (ou transformação ou estimador). Isso vai ser útil para recuperar algumas informações do _pipeline_ mais a frente.\n",
        "* Um objeto da classe do transformador ou estimador, já com seus parâmetros configurados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqthBhA18ITd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_pipeline = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"minmax_scaler\", MinMaxScaler()),\n",
        "    (\"poly_features\", PolynomialFeatures(degree=2, include_bias=False))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UVr1XWCfZID",
        "colab_type": "text"
      },
      "source": [
        "Depois da especificação do nosso _pipeline_, podemos aplicá-lo simultaneamente a diversas variáveis (desde que as transformações especificadas façam sentido).\n",
        "\n",
        "No exemplo abaixo, aplicamos esse _pipeline_ às variáveis `Height` e `Score` ao mesmo tempo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh8kbymmDZqB",
        "colab_type": "code",
        "outputId": "0595019a-1288-4ea8-d18b-1d61dc44136b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "pipeline_transformation = num_pipeline.fit_transform(data_missing[[\"Height\", \"Score\"]])\n",
        "\n",
        "pipeline_transformation[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoNf9vDJfrW8",
        "colab_type": "text"
      },
      "source": [
        "Para ficar mais claro a saída do _pipeline_, podemos utilizar os nomes das _features_ geradas através do método `get_feature_names()`. Para tornar ainda mais claro, substituímos o que é chamado `x0` por `Height` e `x1` por `Score`, que é inferido pela ordem das variáveis no _pipeline_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJz5zvr2EeM3",
        "colab_type": "code",
        "outputId": "444fe35c-4e5e-4f9c-ef6a-152dd9bcd775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "poly_features = num_pipeline.get_params()[\"poly_features\"].get_feature_names()\n",
        "  \n",
        "pipeline_columns = [old_name.replace(\"x0\", \"Height_n\").replace(\"x1\", \"Score_n\") for old_name in poly_features]\n",
        "\n",
        "pipeline_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBgEafF-gKA3",
        "colab_type": "text"
      },
      "source": [
        "Criamos um novo _data set_ com essas variáveis resultantes do _pipeline_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_xBepJGIAJm",
        "colab_type": "code",
        "outputId": "6126947b-ef3f-42db-84aa-4317ed5f79d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "height_score_normalized_poly = pd.DataFrame(pipeline_transformation, columns=pipeline_columns)\n",
        "\n",
        "height_score_normalized_poly.head(6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9imGtnaygRiX",
        "colab_type": "text"
      },
      "source": [
        "Podemos também criar outro _pipeline_ para a variável categórica `Course`. Como se trata de uma variável de natureza completamente diferente, precisamos especificar um _pipeline_ diferente com as seguintes transformações:\n",
        "\n",
        "1. Preencha os dados faltantes (`None`) com a classe `Unknown`.\n",
        "2. Crie novas variáveis binárias com o `OneHotEncoder`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZP_HTkchI5c",
        "colab_type": "text"
      },
      "source": [
        "Assim como no _pipeline_ anterior, especificamos cada etapa como uma tupla com um nome e um objeto de um transformador ou estimador:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMv_2lV7KxTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n",
        "    (\"one_hot_encoder\", OneHotEncoder(sparse=False, dtype=np.int))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK66jYTShV52",
        "colab_type": "text"
      },
      "source": [
        "Após a especificação do _pipeline_, podemos aplicá-lo à nossa variável `Course`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIFWvPS7LNUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "course_pipeline_transformation = cat_pipeline.fit_transform(data_missing[[\"Course\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quJ4ThBBhfBI",
        "colab_type": "text"
      },
      "source": [
        "Agora, utilizaremos o nome que demos à etapa do `OneHotEncoder` para recuperar esse transformador através do método `get_params()`. Depois de recuperado o `OneHotEncoder`, acessamos seu atributo `categories_` (primeiro índice `[0]`, pois poderíamos ter aplicado o _pipeline_ a mais de uma variável categórica):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zurb-NVWM4sX",
        "colab_type": "code",
        "outputId": "1e7c2960-6ffb-4285-bb2d-691157302850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "course_columns = cat_pipeline.get_params()[\"one_hot_encoder\"].categories_[0]\n",
        "\n",
        "course_columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABQDGjU_iDGS",
        "colab_type": "text"
      },
      "source": [
        "Utilizamos a saída do _pipeline_ e os nomes das categorias recuperados do transformador para criar um novo `DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ec56uIcMvll",
        "colab_type": "code",
        "outputId": "5707acac-8d67-4d74-eb02-d73b98f6340a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "course_discretized = pd.DataFrame(course_pipeline_transformation, columns=course_columns)\n",
        "\n",
        "course_discretized.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeO6hmSEiL6N",
        "colab_type": "text"
      },
      "source": [
        "Por fim, combinamos as saídas dos dois _pipelines_ para criar um único `DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8tL_jS1NTf7",
        "colab_type": "code",
        "outputId": "8b39c1c3-e549-4cea-fade-7c8e90d290ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "data_transformed = pd.concat([height_score_normalized_poly, course_discretized], axis=1)\n",
        "\n",
        "data_transformed.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NLD-pyliXWO",
        "colab_type": "text"
      },
      "source": [
        "Vale ressaltar que:\n",
        "\n",
        "* Poderíamos utilizar também o `ColumnTransformer` para compor (por isso, ele se encontra no módulo `sklearn.compose`) múltiplos `Pipeline` em diferentes variáveis.\n",
        "* Os `Pipeline` não servem apenas para a transformação dos dados de treinamento. Eles também podem (e devem) ser usados para submeter os dados de teste e até de produção aos mesmos procedimentos dos dados de treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbShR7kMZGwE",
        "colab_type": "text"
      },
      "source": [
        "## _Outliers_\n",
        "\n",
        "_Outliers_, os famosos \"pontos fora da curva\", são observações que não parecem seguir o mesmo padrão dos demais dados. Eles podem vir de distribuições diferentes, serem erros na coleta de dados, erros de medição etc.\n",
        "\n",
        "Eles influenciam nossas análises e os nossos algoritmos ao apresentar comportamento distoante do resto do _data set_, impactando na média, variância, funções de perda e custo etc. Se fizer sentido, eles devem ser removidos ou transformados antes de prosseguirmos com a análise.\n",
        "\n",
        "No entanto, devemos julgar com cautela sua remoção: __alguns _outliers_ são dados autênticos e devem ser estudados com atenção__. Por exemplo, a remoção de uma medição muito alta na temperatura de um reator seria um erro, pois essa medição pode estar nos indicando um potencial problema com o dispositivo.\n",
        "\n",
        "Abaixo estudamos algumas técnicas simples para encontrar _outliers_.\n",
        "\n",
        "![outlier](https://www.stats4stem.org/common/web/plugins/ckeditor/plugins/doksoft_uploader/userfiles/WithInfOutlier.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3bsTDv0pAN4",
        "colab_type": "text"
      },
      "source": [
        "Começamos criando uma cópia da variável `Height` do nosso _data set_ para não impactar o original:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQ7AQztcZkYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "height_outlier = data.Height.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQNHBAu4pHcp",
        "colab_type": "text"
      },
      "source": [
        "Adicionamos dez  _outliers_ que representam pessoas estranhamente baixas ou estranhamente altas para o padrão que estamos observando:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX2R3V0HZI0w",
        "colab_type": "code",
        "outputId": "6acbd63c-820e-485a-cde4-72a69fefe13d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "height_outlier_idx = pd.Index(np.random.choice(height_outlier.index, 10, replace=False))\n",
        "\n",
        "too_short_idx = pd.Index(height_outlier_idx[:5])\n",
        "too_tall_idx = pd.Index(height_outlier_idx[5:])\n",
        "\n",
        "height_outlier[too_short_idx] = np.random.normal(loc=1.30, scale=0.5, size=5)\n",
        "height_outlier[too_tall_idx] = np.random.normal(loc=2.20, scale=0.5, size=5)\n",
        "\n",
        "outlier_idx = too_short_idx | too_tall_idx\n",
        "\n",
        "height_outlier[outlier_idx]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwNbTzDnpoDL",
        "colab_type": "text"
      },
      "source": [
        "Note que nem todos dados gerados se tornaram realmente _outliers_. Como geramos de uma distribuição aleatória, corremos esse risco.\n",
        "\n",
        "No entanto, temos alguns dados estranhos como 0.51 m e 2.73 m."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5pwD_1EqRNZ",
        "colab_type": "text"
      },
      "source": [
        "No _boxplot_ padrão, os dados mais extremos são mostrados como pontos fora do alcance dos _whiskers_ (as barrinhas do _box plot_).\n",
        "\n",
        "No caso abaixo, notamos três pontos acima e três pontos abaixo do considerado \"dentro da faixa normal\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRMVhYz3b2KH",
        "colab_type": "code",
        "outputId": "9e090cef-804c-4f17-958b-5e25154662db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        }
      },
      "source": [
        "sns.boxplot(height_outlier, orient=\"vertical\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOKP49JMqTog",
        "colab_type": "text"
      },
      "source": [
        "Uma primeira abordagem bem simples é encontrar os pontos do _box plot_ acima.\n",
        "\n",
        "Tudo que estiver fora da faixa $[Q1 - 1.5 \\times \\text{IQR}, Q3 + 1.5 \\times \\text{IQR}]$ é considerado um ponto anômalo para aquele padrão:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_h0zaVDce0N",
        "colab_type": "code",
        "outputId": "86b9e772-6438-4820-87ba-dab83a4b1dd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "q1 = height_outlier.quantile(0.25)\n",
        "q3 = height_outlier.quantile(0.75)\n",
        "iqr = q3 - q1\n",
        "\n",
        "non_outlier_interval_iqr = [q1 - 1.5 * iqr, q3 + 1.5 * iqr]\n",
        "\n",
        "print(f\"Faixa considerada \\\"normal\\\": {non_outlier_interval_iqr}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsuVvr8hq4Rc",
        "colab_type": "text"
      },
      "source": [
        "Agora podemos identificar quais pontos encontram-se fora desse intervalo, ou seja, podem ser considerados _outliers_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm78PWbhc9Dz",
        "colab_type": "code",
        "outputId": "ee3995ea-8a63-4c90-b3dd-57ba673887ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "outliers_iqr = height_outlier[(height_outlier < non_outlier_interval_iqr[0]) | (height_outlier > non_outlier_interval_iqr[1])]\n",
        "\n",
        "outliers_iqr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcF70kmerGEq",
        "colab_type": "text"
      },
      "source": [
        "Se estivermos seguos de que esses pontos representam de fato _outliers_ e que sua remoção não traz prejuízo à nossa análise, então podemos removê-los:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVRJS9DNeb9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "height_no_outlier_iqr = height_outlier.drop(index=outliers_iqr.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urvTyUfHrVrJ",
        "colab_type": "text"
      },
      "source": [
        "Uma segunda abordagem é observar as estatísticas descritivas dos dados.\n",
        "\n",
        "Repare no histograma abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc_paOePfHJ5",
        "colab_type": "code",
        "outputId": "6840da1c-bae6-4465-8aa7-87f69928e182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        }
      },
      "source": [
        "sns.distplot(height_outlier);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI9ToieVrisQ",
        "colab_type": "text"
      },
      "source": [
        "Dá para perceber que a maior parte dos dados concentra-se em torno da média (~ 1.7 m) e que apenas algumas observações encontram-se bastante distantes dela."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q49-oFz4gBHs",
        "colab_type": "code",
        "outputId": "f968b883-a1e3-4ead-963a-19d9f25e9d9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "height_outlier_mean = height_outlier.mean()\n",
        "height_outlier_std = height_outlier.std()\n",
        "\n",
        "height_outlier_mean, height_outlier_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTtLF6P2rvIh",
        "colab_type": "text"
      },
      "source": [
        "Um jeito de procurar por _outliers_ é ver quem se encontra fora do intervalo $[\\bar{x} - k * \\sigma, \\bar{x} + k * \\sigma]$, onde $k$ geralmente é 1.5, 2.0, 2.5 ou até 3.0.\n",
        "\n",
        "Abaixo utilizamos o $k = 2$, pois esse valor faz sentido (alturas menores que 1.12 m ou maiores que 2.30 m fogem do nosso padrão):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI8gL-QrgK1s",
        "colab_type": "code",
        "outputId": "6c472ac1-ea23-4dd3-b833-91969a62f92d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "non_outlier_interval_dist = [height_outlier_mean - 2 * height_outlier_std, height_outlier_mean + 2 * height_outlier_std]\n",
        "\n",
        "non_outlier_interval_dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5A37brPsVPw",
        "colab_type": "text"
      },
      "source": [
        "Novamente, conhecendo o intervalo, podemos identificar as observações que caem foram dele e removê-las:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6jVe5TMglf5",
        "colab_type": "code",
        "outputId": "c270dcb7-d46a-4dd8-94b3-c3d610269282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "outliers_dist = height_outlier[(height_outlier < non_outlier_interval_dist[0]) | (height_outlier > non_outlier_interval_dist[1])]\n",
        "\n",
        "outliers_dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqYD2d3chJTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "height_no_outlier_dist = height_outlier.drop(index=outliers_dist.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IL5fWP1sePM",
        "colab_type": "text"
      },
      "source": [
        "Até agora, nossas métodos de identificação de _outlier_ foram baseadas em estatísticas descritivas do nosso _data set_ (quantis, média e variância). Porém, alguns testes de hipóteses também existem.\n",
        "\n",
        "Um deles é o teste de Grubb. Esse é um teste bastante simples, cuja estatística de teste $G$ depende dos valores extremos do conjunto e da média amostral:\n",
        "\n",
        "$$G = \\frac{\\vert x_{\\text{\\{min ou max\\}}} - \\bar{x}\\vert}{s}$$\n",
        "\n",
        "onde $\\bar{x}$ é a média amostral e $s$ é o desvio-padrão da amostra.\n",
        "\n",
        "A hipótese nula, $H_{0}$, é de que não existem _outliers_ no _data set_. O teste de Grubb assume que os dados originam-se de uma distribuição normal, então pode ser válido testar essa hipótese antes.\n",
        "\n",
        "Rejeitamos a hipótese nula se o valor de $G$ encontrado for superior ao valor crítico do teste, que é dado por\n",
        "\n",
        "$$G_{\\text{crítico}} = \\frac{n - 1}{\\sqrt{n}} \\sqrt{\\frac{t_{\\alpha',n-2}^{2}}{n - 2 + t_{\\alpha',n-2}^{2}}}$$\n",
        "\n",
        "onde $n$ é o tamanho da amostra, $t$ é um valor com distribuição t-Student e $\\alpha'$ é $\\alpha/2n$ se o teste for bilateral (procuramos _outliers_ muito acima ou muito abaixo) ou $\\alpha/n$ se o teste for unilateral (acreditamos que o _outlier_, se houver, está em somente uma das extremidades da distribuição)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNveH7ftxMOV",
        "colab_type": "text"
      },
      "source": [
        "Abaixo criamos algumas funções que nos auxiliam nos cálculos e na exibição dos resultados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir61-q0ckV6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grubb_test(g, n, alpha=0.05, tailed='two-tailed'):\n",
        "  if tailed == 'two-tailed':\n",
        "    critical = ((n - 1)/sqrt(n)) * sqrt(sct.t.isf(alpha/(2*n), n-2)**2/(n - 2 + sct.t.isf(alpha/(2*n), n-2)**2))\n",
        "    \n",
        "    return (g, critical, g > critical)\n",
        "  elif tailed == 'one-tailed':\n",
        "    critical = ((n - 1)/sqrt(n)) * sqrt(sct.t.isf(alpha/(n), n-2)**2/(n - 2 + sct.t.isf(alpha/(n), n-2)**2))\n",
        "    \n",
        "    return (g, critical, g > critical)\n",
        "  else:\n",
        "    raise ValueError(f\"Invalid tailed argument\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c--VvSPuuHaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grubb_summary(result, decimals=10):\n",
        "  return (\n",
        "    f\"Null hypothesis: there is no outliers in the data set\\n\"\n",
        "    f\"Test statistic: {np.round(result[0], decimals)}, \"\n",
        "    f\"Grubb's critical value: {np.round(result[1], decimals)}, \"\n",
        "    f\"Reject: {result[2]}\"\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8nFGEVuqgdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def next_outlier_candidate(data):\n",
        "  sample_distances = (data - data.mean()).abs()\n",
        "  candidate_idx = sample_distances.idxmax()\n",
        "  candidate_value = data[candidate_idx]\n",
        "  candidate_statistic = sample_distances.max()/data.std()\n",
        "  \n",
        "  return (candidate_idx, candidate_value, candidate_statistic, len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRZwuyOOxU7U",
        "colab_type": "text"
      },
      "source": [
        "Ao executarmos o teste de Grubb no nosso conjunto de alturas, encontramos alguns valores onde a hipótese nula é rejeitada, ou seja, há evidência de que o valor extremo é um _outlier_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz-yVWFlt-M6",
        "colab_type": "code",
        "outputId": "cb11e99b-2195-45d7-9089-fdf292a65e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "source": [
        "height_outlier_grubb = height_outlier.copy()\n",
        "outliers_grubb = pd.Series()\n",
        "has_outlier = True\n",
        "\n",
        "while has_outlier:\n",
        "  outlier_candidate = next_outlier_candidate(height_outlier_grubb)\n",
        "\n",
        "  print(f\"Index: {outlier_candidate[0]}, \"\n",
        "        f\"Value: {np.round(outlier_candidate[1], 3)}, \"\n",
        "        f\"Test statistic: {np.round(outlier_candidate[2], 3)}, \"\n",
        "        f\"Sample size: {outlier_candidate[3]}\\n\")\n",
        "\n",
        "  result = grubb_test(outlier_candidate[2], outlier_candidate[3])\n",
        "\n",
        "  print(grubb_summary(result, 3))\n",
        "\n",
        "  has_outlier = result[2]\n",
        "\n",
        "  if has_outlier:\n",
        "    height_outlier_grubb = height_outlier_grubb.drop(index=outlier_candidate[0])\n",
        "    outliers_grubb.at[outlier_candidate[0]] = outlier_candidate[1]\n",
        "    \n",
        "  print(f\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49MMneSg-DCj",
        "colab_type": "code",
        "outputId": "a98df152-223e-43e1-ced9-d113a40b879f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "outliers_grubb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hajYam661Zd",
        "colab_type": "text"
      },
      "source": [
        "Abaixo comparamos os _outliers_ encontrados por cada um dos três métodos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3P2Bavg-zMK",
        "colab_type": "code",
        "outputId": "25065774-49a4-4509-fe92-70a4d32c8cd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "outliers = pd.Series({\"IQR\": outliers_iqr.index.values,\n",
        "                      \"Normal\": outliers_dist.index.values,\n",
        "                      \"Grubb\": outliers_grubb.index.values})\n",
        "\n",
        "outliers.apply(np.sort)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oMEwGs_DHJW",
        "colab_type": "text"
      },
      "source": [
        "## _Features_ de texto\n",
        "\n",
        "Dados textuais são muito ricos e muito fáceis de serem encontrados. Diversos _data sets_ são compostos por documentos textuais e ainda um simples _scrapper_ pode coletar dezenas de milhares de documentos da Internet. Coleções de documentos são frequentemente chamadas de _corpus_ (plural, _corpora_).\n",
        "\n",
        "Nosso objetivo aqui é somente mostrar como preprocessar de forma simples _features_ textuais. Para isso, utilizaremos o _data set_ 20 newsgroups, que contém milhares de documentos categorizados em 20 grupos (desde astronomia até carros)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XItMVwyq8Dp9",
        "colab_type": "text"
      },
      "source": [
        "Abaixo escolhemos somente três grupos para restringir nosso escopo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usWrDfLvMNxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categories = [\"sci.crypt\", \"sci.med\", \"sci.space\"]\n",
        "\n",
        "newsgroups = fetch_20newsgroups(subset=\"train\", categories=categories, shuffle=True, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uNwK5uREAn7",
        "colab_type": "text"
      },
      "source": [
        "Temos agora um _corpus_ com 1782 documentos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lUWgt06EtnR",
        "colab_type": "code",
        "outputId": "f82dd8b7-5f76-477c-9173-ee35d0c7e0aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(newsgroups.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh326fr28Jyc",
        "colab_type": "text"
      },
      "source": [
        "Um exemplo de documento desse _corpus_ é mostrado abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsfaD72_M52H",
        "colab_type": "code",
        "outputId": "fb895197-8753-49e6-a631-e7716ad8c8ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "document_idx = 4\n",
        "documents_total = len(newsgroups.data)\n",
        "\n",
        "print(f\"> Document {document_idx} of {documents_total}:\\n\\n{newsgroups.data[document_idx]}\")\n",
        "print(f\"> Category: {newsgroups.target_names[newsgroups.target[document_idx]]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6liTZFzv8Nas",
        "colab_type": "text"
      },
      "source": [
        "Quando trabalhando com dados textuais, uma representação simples é ter:\n",
        "\n",
        "* Cada documento em uma linha.\n",
        "* Cada palavra (ou termo) em uma coluna.\n",
        "\n",
        "Por exemplo, se nosso vocábulário (conjunto de todas palavras ou termos do _corpus_) tiver tamanho 10000 e tivermos 100 documentos, então nosso _data set_ será composto de 100 linhas e 10000 colunas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLBi7mFU8mLI",
        "colab_type": "text"
      },
      "source": [
        "O valor de cada célula, $x_{i, j}$, (interseção da linha $i$ com a coluna $j$) do _data set_ depende da tranformação que aplicarmos.\n",
        "\n",
        "A transformação mais simples é a contagem de palavras no documento, ou seja, $x_{i, j}$ indica o número de ocorrências da palavra $j$ no documento $i$.\n",
        "\n",
        "Isso pode ser obtido no sklearn pelo `CountVectorizer`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E6FmUUhNs8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vectorizer = CountVectorizer()\n",
        "newsgroups_counts = count_vectorizer.fit_transform(newsgroups.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSylOCPKjLmh",
        "colab_type": "code",
        "outputId": "d7b6e6b8-f227-4ec5-a34a-2cf93fc8ebb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(newsgroups_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4rtFrsF9CgR",
        "colab_type": "text"
      },
      "source": [
        "Abaixo escolhemos dez palavras contidas no _corpus_ para exemplificar:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmxzJhkSUpIZ",
        "colab_type": "code",
        "outputId": "613a8241-c25e-4d5d-9830-1cee04671fc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "words_idx = sorted([count_vectorizer.vocabulary_.get(f\"{word.lower()}\") for word in\n",
        "                    [u\"clipper\", u\"Kapor\",\n",
        "                     u\"monitor\", u\"gibberish\",\n",
        "                     u\"Banks\", u\"private\",\n",
        "                     u\"study\", u\"group\",\n",
        "                     u\"Colorado\", u\"Business\"]])\n",
        "\n",
        "pd.DataFrame(newsgroups_counts[:5, words_idx].toarray(), columns=np.array(count_vectorizer.get_feature_names())[words_idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7WuoRgP9WE9",
        "colab_type": "text"
      },
      "source": [
        "Por exemplo, o valor 2 na interseção do documento 0 com a coluna `clipper` indica que a palavra _clipper_ aparece duas vezes no documento 0. Obviamente é possível que uma mesma palavra apareça em múltiplos documentos e mais óbvio ainda que um documento contenha múltiplas palavras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQzj-_QT9p7e",
        "colab_type": "text"
      },
      "source": [
        "O problema com essa abordagem é que não temos como medir relevância dos termos. E se o  termo é super comum e aparece em quase todos documentos? E se o termo aparece muitas vezes no mesmo documento, mas poucas vezes nos outros?\n",
        "\n",
        "Essas perguntas não podem ser respondidas simplesmente com a contagem de termos acima. Para isso, precisamos do tf-idf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXBnOFk___QK",
        "colab_type": "text"
      },
      "source": [
        "O tf-idf é uma estatística baseada no _corpus_ composta de outras duas estatísticas:\n",
        "\n",
        "* $\\text{tf}(t, d)$, ou _term frequency_, é uma medida de quantas vezes o termo $t$ aparece no documento $d$. Algumas opções estão disponíveis, mas a mais simples é a contagem do número de ocorrências do termo no documento, $f_{t, d}$, exatamente o que computamos acima. Essa é a forma como sklearn define $tf$:\n",
        "\n",
        "$$\\text{tf}(t, d) = f_{t, d}$$\n",
        "\n",
        "* $\\text{idf}(t)$, ou _inverse document frequency_, é uma medida de relevância do termo em todos documentos do _corpus_. O sklearn a computa, seguindo valores _default_, da seguinte forma:\n",
        "\n",
        "$$\\text{idf}(t) = \\log{\\frac{1+n}{1 + d_{t}}} + 1$$\n",
        "\n",
        "onde $n$ é o número de documentos no _corpus_ e $d_{t}$ é o número de documentos no _corpus_ que contêm o termo $t$ ($0 < d_{t} \\leq n$).\n",
        "\n",
        "O tf-idf é calculado multiplicando esses dois valores:\n",
        "\n",
        "$$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t) = f_{t, d} \\times \\log{\\frac{1+n}{1 + d_{t}}} + 1$$\n",
        "\n",
        "O sklearn também normaliza todos documentos resultantes, ou seja todas linhas da matriz, para terem norma unitária. Em outras palavras, os elementos do vetor de tf-idf do documento $i$ são dados por:\n",
        "\n",
        "$$\\text{tf-idf}(i, j)_{\\text{normalizado}} = \\frac{\\text{tf-idf}(i, j)}{\\sqrt{\\text{tf-idf}(i, 1)^{2} + \\text{tf-idf}(i, 2)^{2} + \\cdots + \\text{tf-idf}(i, T)^{2}}}$$\n",
        "\n",
        "onde $T$ é o número de termos do _corpus_, ou seja, o tamanho do vocabulário."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWpYWUMjCH8l",
        "colab_type": "text"
      },
      "source": [
        "O tf-idf é sempre um valor não negativo e quanto mais alto, maior a relevância do termo.\n",
        "\n",
        "Note como o tf aumenta de acordo com o número de ocorrências do termo no documento: quanto mais frequente o termo, mas relevante ele parece ser.\n",
        "\n",
        "O idf é uma medida de \"raridade\" do termo através de todo _corpus_: quanto mais alto, menos o termo aparece no _corpus_ e consequentemente mais informação ele traz.\n",
        "\n",
        "Multiplicando os dois, temos uma medida do quão relevante aquele termo é para aquele documento no _corpus_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_N2VQnwDaey",
        "colab_type": "text"
      },
      "source": [
        "O sklearn provê um transformador, `TfidfTransformer`, que transforma de uma matriz de frequências, como a retornada pelo `CountVectorizer`, e retorna uma matriz de tf-idf:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyxgx0YhVwtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_transformer = TfidfTransformer()\n",
        "\n",
        "tfidf_transformer.fit(newsgroups_counts)\n",
        "\n",
        "newsgroups_tfidf = tfidf_transformer.transform(newsgroups_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evk8smtLWNtO",
        "colab_type": "code",
        "outputId": "bf99b51a-e276-480c-dee9-13713e85a00b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "pd.DataFrame(newsgroups_tfidf[:5, words_idx].toarray(), columns=np.array(count_vectorizer.get_feature_names())[words_idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9hI18kYDsuA",
        "colab_type": "text"
      },
      "source": [
        "Também podemos obter a matriz de tf-idf diretamente do _corpus_ sem ter que passar pela matriz de frequência com o transformador `TfidfVectorizer`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPV4xrxzWlA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "tfidf_vectorizer.fit(newsgroups.data)\n",
        "\n",
        "newsgroups_tfidf_vectorized = tfidf_vectorizer.transform(newsgroups.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAQ20ew-Wx5V",
        "colab_type": "code",
        "outputId": "fd781f7a-198a-444f-bfb8-baee26469ef0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "pd.DataFrame(newsgroups_tfidf_vectorized[:5, words_idx].toarray(), columns=np.array(count_vectorizer.get_feature_names())[words_idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLFGR7A_D0px",
        "colab_type": "text"
      },
      "source": [
        "Note como a matriz acima é exatamente igual a retornada pelo `TfidfTransformer`.\n",
        "\n",
        "O resultado (igual da matriz de frequência) é um _data set_ com 1782 documentos e 33796 termos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I_w7yLeYnRe",
        "colab_type": "code",
        "outputId": "e1162574-03a2-4368-c3b6-517759bb973f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "newsgroups_tfidf_vectorized.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjPMTtkUwrS1",
        "colab_type": "text"
      },
      "source": [
        "## Referências\n",
        "\n",
        "* [Feature engineering](https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html)\n",
        "\n",
        "* [Feature Scaling with scikit-learn](http://benalexkeen.com/feature-scaling-with-scikit-learn/)\n",
        "\n",
        "* [Anthony Goldbloom gives you the secret to winning Kaggle competitions](https://www.import.io/post/how-to-win-a-kaggle-competition/)\n",
        "\n",
        "* [What are some best practices in Feature Engineering?](https://www.quora.com/What-are-some-best-practices-in-Feature-Engineering)\n",
        "\n",
        "* [Discover Feature Engineering, How to Engineer Features and How to Get Good at It](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)\n",
        "\n",
        "* [Fundamental Techniques of Feature Engineering for Machine Learning](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114)\n",
        "\n",
        "* [Feature Engineering Cookbook for Machine Learning](https://medium.com/@michaelabehsera/feature-engineering-cookbook-for-machine-learning-7bf21f0bcbae)\n",
        "\n",
        "* [A Simple Guide to Scikit-learn Pipelines](https://medium.com/vickdata/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf)\n",
        "\n",
        "* [Outlier detection with Scikit Learn](https://www.mikulskibartosz.name/outlier-detection-with-scikit-learn/)\n",
        "\n",
        "* [Working With Text Data](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
        "\n",
        "* [WTF is TF-IDF?](https://www.kdnuggets.com/2018/08/wtf-tf-idf.html)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Aula 7 - Feature Engineering.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python38264bitvenvvenv6b7957e303954303a87cb14f5a24811f",
      "display_name": "Python 3.8.2 64-bit ('venv': venv)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}